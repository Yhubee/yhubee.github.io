<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research &amp; Writing | Ubong Eno Nkereuwem</title>
  <meta name="description" content="Research themes on human–AI interaction, trust calibration, adaptive explanations, and sociotechnical fairness." />
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <header>
    <div class="navbar">
      <div class="brand">Ubong Eno Nkereuwem</div>
      <nav aria-label="Primary">
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="about.html">About</a></li>
          <li><a href="projects.html">Projects</a></li>
          <li><a class="active" href="research.html">Research</a></li>
          <li><a href="publications.html">Publications</a></li>
          <li><a href="teaching.html">Teaching</a></li>
          <li><a href="why.html">Why I Do This Work</a></li>
          <li><a href="bio.html">Bio</a></li>
        </ul>
      </nav>
    </div>
  </header>
  <main class="container">
    <h1>Research &amp; Writing</h1>
    <p>My research examines how people interpret, trust, and collaborate with intelligent systems, and how sociotechnical design influences autonomy, judgment, and value alignment over time. I am particularly interested in contexts where users face uncertainty, emotional stakes, and information asymmetry—conditions that amplify the influence of AI recommendations.</p>
    <p>I approach Human–Centered AI through a mixed-methods lens that integrates cognitive psychology, behavioral science, HCI theory, and ethical design principles. My work is shaped by applied experience in EdTech, accessibility, and dataset construction; academic training in research and business management; and lived experience within diverse educational and community contexts.</p>

    <div class="section-divider"></div>

    <section>
      <h2>Research Themes</h2>
      <article class="card">
        <h3>Mental Model Formation in Human–AI Interaction</h3>
        <p>I explore how confidence signals, recommendation framing, system transparency, explanation style, environmental uncertainty, and cultural or cognitive context shape the mental models that users build about system competence, limitations, and intent. A striking applied observation involved a neurodivergent user implicitly trusting an AI system because it appeared confident—illuminating subtle pathways through which AI can displace human judgment.</p>
      </article>
      <article class="card">
        <h3>Trust Calibration and Autonomy Preservation</h3>
        <p>Trust in AI systems is often miscalibrated—over-trust when users assume correctness based on perceived authority, and under-trust when lack of transparency causes unnecessary skepticism. I study trust drift across repeated interactions, behavioral signals of over- or under-reliance, and design patterns that strengthen user sense of control. This aligns with my professional work building value-sensitive defaults, user-managed settings, rationale explanations, and guardrails within AI-guided advising systems.</p>
      </article>
      <article class="card">
        <h3>Adaptive Explanation Design</h3>
        <p>Not all users benefit from the same explanation style or depth. I investigate cognitively adaptive explanations, culturally aware framing, how uncertainty disclosure affects comprehension, when to offer multi-pathway reasoning, and how to support reflection without overwhelming users—connecting directly to my work designing explanation schemas for EdTech systems.</p>
      </article>
      <article class="card">
        <h3>Sociotechnical Fairness in AI-Supported Decision Systems</h3>
        <p>I study how AI nudges shape opportunity access, how predictive systems influence choice architecture, where bias can emerge from dataset construction, and how algorithmic recommendations intersect with cultural and structural inequities. My goal is to develop fairness frameworks that are contextually aware and ethically interpretable.</p>
      </article>
      <article class="card">
        <h3>Longitudinal Impact of Transparency and Ethical UX Interventions</h3>
        <p>I examine methods for measuring autonomy drift, trust calibration trajectories, value-alignment stability, and cognitive fatigue or over-reliance over long-term system use—addressing the gap between ethical intention and measurable impact.</p>
      </article>
    </section>

    <div class="section-divider"></div>

    <section>
      <h2>Graduate Research (University of Roehampton)</h2>
      <p class="meta">Topic: Digital Ordering, Perceived Fairness, and Time Transparency · Methods: Surveys, thematic analysis, contextual inquiry, service blueprinting</p>
      <p>My MSc dissertation investigated how digital service interfaces construct perceptions of fairness and credibility in semi-automated food pickup systems. I was motivated by the insight that users’ perceptions of waiting, order flow, and system reliability depend far more on interface cues and transparency structures than on actual operational time.</p>
      <ul class="list">
        <li>Identified that perceived wait time is a sociotechnical construct influenced by transparency, predictability, and environmental cues</li>
        <li>Conducted mixed-method research integrating qualitative interviews with descriptive analysis</li>
        <li>Developed a redesigned service blueprint projected to reduce perceived wait time by 35 percent</li>
        <li>Highlighted the role of psychological expectation management in service trust</li>
        <li>Mapped how ambiguous or missing feedback cues produce distrust and negative mental models</li>
        <li>Proposed an improved service design projected to reduce perceived wait time by ~35%</li>
      </ul>
      <p>This project grounded my understanding of sociotechnical fairness and clarified my interest in psychological constructs such as expectation management and time perception in digital systems.</p>
    </section>

    <section>
      <h2>Undergraduate Research (University of Benin)</h2>
      <p class="meta">Globalization and Local Development Dynamics</p>
      <p>Earlier research explored how global economic systems shape development outcomes in Akwa Ibom State. This project seeded my interest in structural inequities, decision systems shaped by policy, and community-level impacts of large-scale systems, providing a foundational lens for understanding sociotechnical and socio-economic systems long before I entered HCI.</p>
    </section>

    <div class="section-divider"></div>

    <section>
      <h2>Applied Research Work</h2>
      <p>My industry-aligned research threads include AI-guided learning and advising systems at Jourhney, where I lead research on decision-support systems that help international students navigate academic choices, visa constraints, funding pathways, and cultural transitions.</p>
      <p>I continue to connect applied and academic inquiry through writing, conceptual framing, and prototyping focused on trust-aware design, adaptive explanations, and autonomy-preserving interactions.</p>
    </section>
  </main>
  <footer class="footer">
    <div class="container">
      <div class="small">© Ubong Eno Nkereuwem</div>
      <div class="small">Email: <a href="mailto:ubongenonkereuwem@gmail.com">ubongenonkereuwem@gmail.com</a> · <a href="https://www.linkedin.com/in/ubongeno" target="_blank" rel="noreferrer">LinkedIn</a></div>
    </div>
  </footer>
</body>
</html>
