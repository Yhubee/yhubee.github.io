<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Research and Writing by Ubong Eno Nkereuwem - Human-AI interaction, trust calibration, and sociotechnical systems research.">
    <meta name="author" content="Ubong Eno Nkereuwem">
    <title>Research & Writing | Ubong Eno Nkereuwem</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <a href="index.html" class="logo">Ubong Eno Nkereuwem</a>
            <nav>
                <a href="index.html">Home</a>
                <a href="about.html">About</a>
                <a href="projects.html">Projects</a>
                <a href="research.html" class="active">Research</a>
                <a href="teaching.html">Teaching</a>
                <a href="why.html">Why This Work</a>
            </nav>
        </div>
    </header>

    <div class="page-header">
        <div class="container">
            <h1>Research & Writing</h1>
            <p class="lead">Conceptual work, applied research, and emerging lines of inquiry</p>
        </div>
    </div>

    <main>
        <div class="container">
            <div class="content-wrapper">
                <nav class="toc">
                    <h3>On This Page</h3>
                    <ul>
                        <li><a href="#research-themes">Research Themes</a></li>
                        <li><a href="#graduate-research">Graduate Research</a></li>
                        <li><a href="#applied-research-work">Applied Research Work</a></li>
                        <li><a href="#themes-under-development">Research Themes Under Development</a></li>
                        <li><a href="#methodological-approach">Methodological Approach</a></li>
                        <li><a href="#theoretical-questions">Working Theoretical Questions</a></li>
                        <li><a href="#long-term-vision">Long-Term Research Vision</a></li>
                    </ul>
                </nav>

                <p>My research examines how people interpret, trust, and collaborate with intelligent systems, and how sociotechnical design influences autonomy, judgment, and value alignment over time. I am particularly interested in contexts where users face uncertainty, emotional stakes, and information asymmetry—conditions that amplify the influence of AI recommendations.</p>

                <p>I approach Human–Centered AI through a mixed-methods lens that integrates cognitive psychology, behavioral science, HCI theory, and ethical design principles. My work is shaped by applied experience in EdTech, accessibility, and dataset construction; academic training in research and business management; and lived experience within diverse educational and community contexts.</p>

                <hr>

                <h2 id="research-themes">Research Themes</h2>

                <div class="theme-card">
                    <h3>1. Mental Model Formation in Human–AI Interaction</h3>
                    <p>A central line of inquiry in my work explores how users form and update mental models of AI systems. I study how:</p>
                    <ul>
                        <li>confidence signals</li>
                        <li>framing of recommendations</li>
                        <li>system transparency</li>
                        <li>explanation style</li>
                        <li>environmental uncertainty</li>
                        <li>cultural and cognitive context</li>
                    </ul>
                    <p>shape the mental models that users build about system competence, limitations, and intent.</p>
                    <p>This interest began with a striking applied observation: in a test session, a neurodivergent user trusted an AI system implicitly—even when its suggestions contradicted her preferences—because the system appeared "confident." This moment illuminated the subtle cognitive pathways through which AI can displace human judgment.</p>
                    <p>I explore how design interventions—such as transparency cues, qualified suggestions, or revealed limitations—can support more accurate mental models.</p>
                </div>

                <div class="theme-card">
                    <h3>2. Trust Calibration and Autonomy Preservation</h3>
                    <p>Trust in AI systems is often miscalibrated:</p>
                    <ul>
                        <li><strong>over-trust</strong> when users assume correctness based on perceived authority</li>
                        <li><strong>under-trust</strong> when lack of transparency causes unnecessary skepticism</li>
                    </ul>
                    <p>My research focuses on building interfaces and interaction strategies that encourage calibrated trust—the alignment of user confidence with system competence.</p>
                    <p>I study:</p>
                    <ul>
                        <li>trust drift across repeated interactions</li>
                        <li>behavioral signals indicating over- or under-reliance</li>
                        <li>how interface framing and system behavior influence autonomy</li>
                        <li>design patterns that strengthen user sense of control</li>
                    </ul>
                    <p>This aligns with my professional work building value-sensitive defaults, user-managed settings, rationale explanations, and guardrails within AI-guided advising systems.</p>
                </div>

                <div class="theme-card">
                    <h3>3. Adaptive Explanation Design</h3>
                    <p>Not all users benefit from the same explanation style or depth. My research investigates:</p>
                    <ul>
                        <li>cognitively adaptive explanations</li>
                        <li>culturally aware explanation framing</li>
                        <li>how uncertainty disclosure affects comprehension</li>
                        <li>when to offer multi-pathway reasoning</li>
                        <li>how to support reflection without overwhelming users</li>
                    </ul>
                    <p>This connects to my work designing explanation schemas for EdTech systems that adjust based on question type, risk level, user uncertainty, and narrative cues.</p>
                </div>

                <div class="theme-card">
                    <h3>4. Sociotechnical Fairness in AI-Supported Decision Systems</h3>
                    <p>Much of my research concerns decision-support systems in education—an emotionally and socially consequential domain.</p>
                    <p>I study:</p>
                    <ul>
                        <li>how AI nudges shape opportunity access</li>
                        <li>how predictive systems influence choice architecture</li>
                        <li>where bias can emerge from dataset construction (CSR labeling work)</li>
                        <li>how algorithmic recommendations intersect with cultural and structural inequities</li>
                    </ul>
                    <p>My work aims to develop frameworks for sociotechnical fairness that are not only mathematically grounded but also contextually aware and ethically interpretable.</p>
                </div>

                <div class="theme-card">
                    <h3>5. Longitudinal Impact of Transparency and Ethical UX Interventions</h3>
                    <p>A rarely studied aspect of Human–Centered AI is the long-term effect of transparency interventions.</p>
                    <p>Industry often implements:</p>
                    <ul>
                        <li>explanations</li>
                        <li>clickable rationales</li>
                        <li>ethical defaults</li>
                        <li>recommended pathways</li>
                        <li>user settings for AI control</li>
                    </ul>
                    <p>but rarely measures how these affect user behavior over time.</p>
                    <p>My research addresses this gap by exploring:</p>
                    <ul>
                        <li>methods for measuring autonomy drift</li>
                        <li>trust calibration trajectories</li>
                        <li>value-alignment stability</li>
                        <li>cognitive fatigue and over-reliance over long-term system use</li>
                    </ul>
                    <p>This theme emerges directly from a core insight: <strong>bridging the gap between ethical intention and measurable impact.</strong></p>
                </div>

                <hr>

                <h2 id="graduate-research">Graduate Research</h2>

                <div class="card">
                    <h3>University of Roehampton (MSc, Distinction)</h3>
                    <p><strong>Topic:</strong> Digital Ordering, Perceived Fairness, and Time Transparency</p>
                    <p><strong>Methods:</strong> Surveys, thematic analysis, contextual inquiry, service blueprinting</p>
                    <p>My MSc dissertation investigated how digital service interfaces construct perceptions of fairness and credibility in semi-automated food pickup systems. I was motivated by the insight that users' perceptions of waiting, order flow, and system reliability depend far more on interface cues and transparency structures than on actual operational time.</p>
                    <h4>Key Research Contributions:</h4>
                    <ul>
                        <li>Identified that perceived wait time is a sociotechnical construct influenced by transparency, predictability, and environmental cues</li>
                        <li>Conducted mixed-method research integrating qualitative interviews with descriptive analysis</li>
                        <li>Developed a redesigned service blueprint projected to reduce perceived wait time by 35 percent</li>
                        <li>Highlighted the role of psychological expectation management in service trust</li>
                        <li>Identified the disconnect between physical service realities and digital representations of queue management</li>
                        <li>Demonstrated how perceived fairness emerges from transparency, consistency, and expectation-setting</li>
                        <li>Mapped how ambiguous or missing feedback cues produce distrust and negative mental models</li>
                    </ul>
                    <p>This project grounded my understanding of sociotechnical fairness and clarified my interest in psychological constructs such as expectation management and time perception in digital systems.</p>
                </div>

                <div class="card">
                    <h3>University of Benin (Undergraduate Research)</h3>
                    <p><strong>Topic:</strong> Globalization and Local Development Dynamics</p>
                    <p>Earlier research explored how global economic systems shape development outcomes in Akwa Ibom State. This project seeded my interest in:</p>
                    <ul>
                        <li>structural inequities</li>
                        <li>decision systems shaped by policy</li>
                        <li>community-level impacts of large-scale systems</li>
                    </ul>
                    <p>It provided a foundational lens for understanding sociotechnical and socio-economic systems long before I entered HCI.</p>
                </div>

                <hr>

                <h2 id="applied-research-work">Applied Research Work</h2>

                <div class="card">
                    <h3>AI-Guided Learning & Advising Systems (Jourhney)</h3>
                    <p>At Jourhney, I lead research on AI-guided decision-support systems that help international students navigate academic choices, visa constraints, funding pathways, and cultural transitions.</p>
                    <p>This work revealed how vulnerable users—uncertain, overloaded, or unfamiliar with local systems—may over-trust AI recommendations. I designed interventions including:</p>
                    <ul>
                        <li>rationale surfacing</li>
                        <li>limited-knowledge disclosures</li>
                        <li>user-managed settings</li>
                        <li>value-sensitive defaults</li>
                        <li>alternative-pathway explanations</li>
                        <li>cognitive offloading support</li>
                    </ul>
                    <p>My applied research has contributed to understanding autonomy drift, confidence mismatch, and the subtle displacement of human reasoning by AI.</p>
                </div>

                <div class="card">
                    <h3>AI Accessibility & Adaptive Personalization (Sky UK)</h3>
                    <p><strong>Methods:</strong> Accessibility research, regulatory analysis (WCAG/Ofcom), inclusive design</p>
                    <p>In my professional certificate capstone, I collaborated with Sky to imagine an AI-powered accessibility feature tailored to sensory, cognitive, and linguistic needs.</p>
                    <p>Research-driven contributions included:</p>
                    <ul>
                        <li>Mapping accessibility barriers across diverse cognitive profiles</li>
                        <li>Designing adaptive content modification features aligned with user control</li>
                        <li>Evaluating the ethical boundary between personalization and paternalism</li>
                        <li>Aligning prototypes to WCAG and Ofcom regulatory frameworks</li>
                    </ul>
                    <p>This experience expanded my interest in inclusive AI, cross-cultural personalization, and the ethical implications of automated adaptation.</p>
                </div>

                <div class="card">
                    <h3>CSR Dataset Annotation & Ethical Data Work (Isahit)</h3>
                    <p><strong>Methods:</strong> Annotation studies, thematic categorization, bias identification</p>
                    <p>My work in annotating CSR-related content for AI model training introduced me to challenges surrounding:</p>
                    <ul>
                        <li>labeling ambiguity</li>
                        <li>annotation consistency</li>
                        <li>dataset bias</li>
                        <li>the interpretive labor that shapes AI behavior</li>
                    </ul>
                    <p>This deepened my sensitivity to the invisible human processes behind "ground truth" datasets and reinforced my interest in interpretability and dataset ethics.</p>
                </div>

                <div class="card">
                    <h3>Development Research (Dataville Research Institute)</h3>
                    <p>My research at Dataville examined how ICT and data-driven systems influence social development outcomes. I analyzed the dynamics between:</p>
                    <ul>
                        <li>policy frameworks</li>
                        <li>data collection practices</li>
                        <li>AI-driven development initiatives</li>
                        <li>digital equity and access</li>
                    </ul>
                    <p>This experience expanded my interest in global HCI, especially in communities with infrastructural, linguistic, or socioeconomic barriers.</p>
                </div>

                <hr>

                <h2 id="themes-under-development">Research Themes Under Development</h2>

                <div class="interests-grid">
                    <div class="interest-card">
                        <h4>Autonomy Drift in Human–AI Collaboration</h4>
                        <p>How repeated AI interactions influence confidence, agency, and self-efficacy.</p>
                    </div>
                    <div class="interest-card">
                        <h4>Explanation Timing, Depth, and Framing</h4>
                        <p>Which explanation structures reduce blind reliance while still supporting clarity and cognitive ease.</p>
                    </div>
                    <div class="interest-card">
                        <h4>Value Alignment as a Dynamic Construct</h4>
                        <p>How user needs, moral reasoning, and goals change, and how AI can adapt responsibly.</p>
                    </div>
                    <div class="interest-card">
                        <h4>The Ethics of Guidance Systems in Education</h4>
                        <p>How students interpret algorithmic nudges and weigh them against personal goals.</p>
                    </div>
                    <div class="interest-card">
                        <h4>Longitudinal Evaluation Methods for Human–AI Interaction</h4>
                        <p>Developing techniques to measure autonomy, trust calibration, and cognitive shifts over time.</p>
                    </div>
                </div>

                <hr>

                <h2 id="methodological-approach">Methodological Approach</h2>
                <p>My research integrates human–centered inquiry, behavioral analysis, and light computational prototyping. Core competencies include:</p>

                <div class="card">
                    <h4>Qualitative Methods</h4>
                    <ul>
                        <li>Semi-structured interviews</li>
                        <li>Thematic and inductive coding</li>
                        <li>Diary and longitudinal studies</li>
                        <li>Usability testing</li>
                        <li>Contextual inquiry</li>
                        <li>Journey mapping</li>
                    </ul>
                </div>

                <div class="card">
                    <h4>Quantitative Methods</h4>
                    <ul>
                        <li>Survey design</li>
                        <li>Descriptive and inferential analysis</li>
                        <li>Likert and psychometric considerations</li>
                        <li>Behavioral pattern analysis</li>
                        <li>Simple A-B comparison studies</li>
                    </ul>
                </div>

                <div class="card">
                    <h4>Sociotechnical & Design Methods</h4>
                    <ul>
                        <li>Value-sensitive design</li>
                        <li>Ethical UX intervention design</li>
                        <li>Trust calibration frameworks</li>
                        <li>Explanation architecture prototyping</li>
                        <li>Service blueprinting</li>
                        <li>Accessibility evaluation</li>
                    </ul>
                </div>

                <div class="card">
                    <h4>Technical Skills (Applied to Research)</h4>
                    <ul>
                        <li>Python for analysis and prototyping</li>
                        <li>SQL for interaction log mining</li>
                        <li>Low-fidelity interface prototyping (Figma, HTML/CSS/JS)</li>
                        <li>Annotation analysis and dataset critique</li>
                    </ul>
                </div>

                <hr>

                <h2 id="theoretical-questions">Working Theoretical Questions</h2>
                <p>These are the guiding inquiries shaping my developing research agenda:</p>
                <ul>
                    <li>How do humans decide when to trust or question an intelligent system?</li>
                    <li>How do mental models of AI evolve over repeated interactions?</li>
                    <li>How can explanation structures be made sensitive to user context, risk, and goals?</li>
                    <li>What design cues prevent over-reliance without overwhelming users with complexity?</li>
                    <li>How can we measure the long-term impact of transparency interventions?</li>
                    <li>How does AI influence opportunity access, especially for international students and learners navigating opaque systems?</li>
                    <li>What does "value alignment" mean in everyday, non-expert AI interactions?</li>
                </ul>

                <hr>

                <h2 id="long-term-vision">Long-Term Research Vision</h2>
                <p>I aim to develop adaptive trust frameworks and evaluative methods that help designers and organizations build AI systems that:</p>
                <ul>
                    <li>support informed decision-making</li>
                    <li>prevent over-reliance</li>
                    <li>protect human judgment</li>
                    <li>adapt to cultural, cognitive, and contextual diversity</li>
                    <li>make system behavior legible and safe</li>
                    <li>respect and preserve human autonomy</li>
                </ul>

                <div class="summary-box">
                    <h3>Ultimate Goal</h3>
                    <p>To establish a <strong>Human–Centered AI Research and Policy Lab</strong> focused on: adaptive trust frameworks, international EdTech fairness, AI accessibility for neurodivergent users, sociotechnical evaluation methods, and ethical algorithmic governance.</p>
                    <p>My career goal is to establish a research lab focused on Human–Centered AI for education, accessibility, and sociotechnical fairness—bridging academia, industry, and public policy.</p>
                </div>

                <div class="contact-info">
                    <h3>Let's Connect</h3>
                    <p>
                        <a href="mailto:ubongenonkereuwem@gmail.com">ubongenonkereuwem@gmail.com</a>
                        <br>
                        <a href="https://www.linkedin.com/in/ubongeno" target="_blank" rel="noopener noreferrer">LinkedIn: linkedin.com/in/ubongeno</a>
                    </p>
                </div>
            </div>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2024 Ubong Eno Nkereuwem. All rights reserved.</p>
            <p>
                <a href="mailto:ubongenonkereuwem@gmail.com">Email</a> · 
                <a href="https://www.linkedin.com/in/ubongeno" target="_blank" rel="noopener noreferrer">LinkedIn</a>
            </p>
        </div>
    </footer>
</body>
</html>
