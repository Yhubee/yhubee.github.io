<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Case study: Human–AI Learning Companion for International Students by Ubong Eno Nkereuwem.">
    <meta name="author" content="Ubong Eno Nkereuwem">
    <title>Human–AI Learning Companion | Ubong Eno Nkereuwem</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <a class="skip-link" href="#main-content">Skip to content</a>
    <header>
        <div class="container">
            <a href="index.html" class="logo">Ubong Eno Nkereuwem</a>
            <nav role="navigation" aria-label="Primary">
                <a href="index.html">Home</a>
                <a href="about.html">About</a>
                <a href="projects.html" class="active" aria-current="page">Projects</a>
                <a href="research.html">Research</a>
                <a href="teaching.html">Teaching</a>
                <a href="why.html">Why This Work</a>
                <a href="index.html#contact">Contact</a>
            </nav>
        </div>
    </header>

    <div class="page-header">
        <div class="container">
            <h1>Human–AI Learning Companion for International Students</h1>
            <p class="lead">Designing explanations and controls that preserve learner agency while offering intelligent guidance.</p>
        </div>
    </div>

    <main id="main-content">
        <div class="container">
            <div class="content-wrapper">
                <section class="project-hero">
                    <p class="project-role"><strong>Role:</strong> Technical AI Product Manager &amp; Human–Centered AI Researcher</p>
                    <p class="project-context"><strong>Context:</strong> AI learning companion for global students facing complex academic choices</p>
                    <p class="project-domains"><strong>Domains:</strong> Decision Support, Explanation Design, Trust Calibration, Digital Learning Ecosystems</p>
                    <p class="project-methods"><strong>Methods:</strong> Semi-structured interviews, cognitive mapping, interaction prototyping, behavioral analysis, explainability modeling</p>
                </section>

                <figure class="project-visual">
                    <img src="content/assets/project-learning-companion.svg" alt="Illustration of a student and AI companion collaborating on coursework">
                    <figcaption>AI learning companion concepts that scaffold research and decision-making while preserving learner autonomy.</figcaption>
                </figure>

                <p class="back-to-projects">← <a href="projects.html">Back to Projects &amp; Case Studies</a></p>

                <section class="project-section">
                    <h2>Overview</h2>
                    <p>International students face complex educational decisions—program selection, visa processes, academic expectations, and career pathways—all while navigating unfamiliar cultural and institutional systems. These decisions involve uncertainty, high stakes, and emotional burden.</p>
                    <p>My work on the AI Learning Companion examined how intelligent guidance systems can support clarity and confidence without inducing over-reliance. The project focused on explanation patterns and interaction boundaries that reinforce user judgment.</p>
                </section>

                <section class="project-section">
                    <h2>Problem Framing</h2>
                    <p>Most digital advising systems present recommendations without transparency. Students experiencing stress or limited familiarity with local academic structures often accept advice unquestioningly. The risk is twofold:</p>
                    <ol>
                        <li>Over-reliance due to cognitive load, perceived authority, or system confidence</li>
                        <li>Erosion of agency when recommendations subtly nudge students toward predefined pathways</li>
                    </ol>
                    <p>My goal was to design an AI system that supports, not supplants, human judgment.</p>
                </section>

                <section class="project-section">
                    <h2>What I Did</h2>
                    <h3>Researching real help-seeking behaviors</h3>
                    <p>I conducted interviews with international students to map uncertainty points, mental models of institutional processes, expectations of AI guidance, and the perceived emotional stakes of decisions.</p>
                    <p>Students frequently interpreted AI confidence as correctness, especially when overwhelmed. This echoed patterns I observed elsewhere—such as Maria, an autistic user who gradually deferred to an AI system's confident recommendations—highlighting the universality of confidence bias in AI-mediated decision making.</p>

                    <h3>Designing adaptive explanation patterns</h3>
                    <p>I co-developed an explanation schema that adjusted based on question type, risk level, user uncertainty signals, amount of context provided, and whether the user asked for justification or alternatives.</p>
                    <p>Examples included "reasoned recommendations," "confidence-qualified suggestions," "multiple-pathway explanations," and "limited knowledge disclosures" to support healthy skepticism and informed choice.</p>

                    <h3>Introducing autonomy-preserving interaction boundaries</h3>
                    <p>I implemented features such as user-managed control settings, value-sensitive defaults, rationale prompts, calls to double-check institutional requirements, and invitations to reflect (“Does this align with what you want?”). These were grounded in principles of FATE, sensemaking, and human–AI complementarity.</p>
                </section>

                <section class="project-section">
                    <h2>Key Artifacts</h2>
                    <div class="artifact-grid">
                        <div class="card artifact-card">
                            <div class="artifact-icon" aria-hidden="true">JM</div>
                            <h4>User journey map (placeholder)</h4>
                            <ul>
                                <li>Mapped pre-arrival stressors and decision checkpoints</li>
                                <li>Highlighted where AI guidance intersected with visa and funding questions</li>
                                <li>Surfaced moments where autonomy felt most fragile</li>
                            </ul>
                        </div>
                        <div class="card artifact-card">
                            <div class="artifact-icon" aria-hidden="true">WF</div>
                            <h4>Wireflows &amp; interaction sketches (placeholder)</h4>
                            <ul>
                                <li>Explored opt-in explanation depth and rationale prompts</li>
                                <li>Tested confidence-qualified suggestions and alternative pathways</li>
                                <li>Documented reflection nudges that reduced blind acceptance</li>
                            </ul>
                        </div>
                        <div class="card artifact-card">
                            <div class="artifact-icon" aria-hidden="true">SB</div>
                            <h4>Interview synthesis board (placeholder)</h4>
                            <ul>
                                <li>Clustered help-seeking behaviors across cultural contexts</li>
                                <li>Linked emotions like uncertainty to reliance patterns</li>
                                <li>Prioritized design responses for transparency and agency</li>
                            </ul>
                        </div>
                    </div>
                    <div class="note-box">
                        <p>Across artifacts, the most impactful shift came from making system limitations visible while inviting students to articulate their own goals alongside the AI’s suggestions.</p>
                    </div>
                </section>

                <section class="project-section">
                    <h2>Outcomes</h2>
                    <ul>
                        <li>Increased student engagement with learning and advising content</li>
                        <li>Greater user awareness of system limitations</li>
                        <li>Reduced blind acceptance of system recommendations</li>
                        <li>Early evidence of trust calibration as users questioned recommendations more thoughtfully</li>
                        <li>Identification of behavioral clusters that now inform adaptive UI/UX flows</li>
                    </ul>
                </section>

                <section class="project-section">
                    <h2>Scholarly Contribution</h2>
                    <p>This project refined my research agenda on trust drift over repeated human–AI interactions, explanation strategies tailored to user cognition, longitudinal measurement of autonomy and self-efficacy, and sociotechnical design for opportunity access.</p>
                </section>

                <p class="back-to-projects">← <a href="projects.html">Back to Projects &amp; Case Studies</a></p>
            </div>
        </div>
    </main>

    <div data-include="footer.html"></div>
    <script src="scripts.js"></script>
</body>
</html>
