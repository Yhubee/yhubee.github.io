<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Case study: Human–AI Learning Companion for International Students by Ubong Eno Nkereuwem.">
    <meta name="author" content="Ubong Eno Nkereuwem">
    <title>Human–AI Learning Companion | Ubong Eno Nkereuwem</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <a class="skip-link" href="#main-content">Skip to content</a>
    <header>
        <div class="container">
            <a href="index.html" class="logo">Ubong Eno Nkereuwem</a>
            <nav role="navigation" aria-label="Primary">
                <a href="index.html">Home</a>
                <a href="about.html">About</a>
                <a href="projects.html" class="active" aria-current="page">Projects</a>
                <a href="research.html">Research</a>
                <a href="teaching.html">Teaching</a>
            </nav>
        </div>
    </header>

    <div class="page-header">
        <div class="container">
            <h1>Human–AI Learning Companion for International Students</h1>
            <p class="lead">Designing explanations and controls that preserve learner agency while offering intelligent guidance.</p>
        </div>
    </div>

    <main id="main-content">
        <div class="container">
            <div class="content-wrapper">
                <section class="project-hero">
                    <p class="project-role"><strong>Role:</strong> Technical AI Product Manager &amp; Human–Centered AI Researcher</p>
                    <p class="project-context"><strong>Context:</strong> AI learning companion for global students facing complex academic choices</p>
                    <p class="project-domains"><strong>Domains:</strong> Decision Support, Explanation Design, Trust Calibration, Digital Learning Ecosystems</p>
                    <p class="project-methods"><strong>Methods:</strong> Semi-structured interviews, cognitive mapping, interaction prototyping, behavioral analysis, explainability modeling</p>
                </section>

                <p class="back-to-projects">← <a href="projects.html">Back to Projects &amp; Case Studies</a></p>

                <section class="project-section">
                    <h2>Overview</h2>
                    <p>International students face complex educational decisions, including program selection, visa processes, academic expectations, and career pathways, all while navigating unfamiliar cultural and institutional systems. These decisions involve uncertainty, high stakes, and emotional burden.</p>
                    <p>My work on the AI Learning Companion examined how intelligent guidance systems can support clarity and confidence without inducing over-reliance. The project focused on explanation patterns and interaction boundaries that reinforce user judgment.</p>
                </section>

                <section class="project-section">
                    <h2>Problem Framing</h2>
                    <p>Most digital advising systems present recommendations without transparency. Students experiencing stress or limited familiarity with local academic structures often accept advice unquestioningly. The risk is twofold:</p>
                    <ol>
                        <li>Over-reliance due to cognitive load, perceived authority, or system confidence</li>
                        <li>Erosion of agency when recommendations subtly nudge students toward predefined pathways</li>
                    </ol>
                    <p>My goal was to design an AI system that supports, not supplants, human judgment.</p>
                </section>

                <section class="project-section">
                    <h2>What I Did</h2>
                    <h3>Researching real help-seeking behaviors</h3>
                    <p>I conducted interviews with international students to map uncertainty points, mental models of institutional processes, expectations of AI guidance, and the perceived emotional stakes of decisions.</p>
                    <p>Students frequently interpreted AI confidence as correctness, especially when overwhelmed. This echoed patterns I observed in other settings, highlighting the universality of confidence bias in AI-mediated decision making.</p>

                    <h3>Designing adaptive explanation patterns</h3>
                    <p>I co-developed an explanation schema that adjusted based on question type, risk level, user uncertainty signals, amount of context provided, and whether the user asked for justification or alternatives.</p>
                    <p>Examples included "reasoned recommendations," "confidence-qualified suggestions," "multiple-pathway explanations," and "limited knowledge disclosures" to support healthy skepticism and informed choice.</p>

                    <h3>Introducing autonomy-preserving interaction boundaries</h3>
                    <p>I implemented features such as user-managed control settings, value-sensitive defaults, rationale prompts, calls to double-check institutional requirements, and invitations to reflect (“Does this align with what you want?”). These were grounded in principles of FATE, sensemaking, and human–AI complementarity.</p>
                </section>

                <section class="project-section">
                    <h2>Outcomes</h2>
                    <ul>
                        <li>Increased student engagement with learning and advising content</li>
                        <li>Greater user awareness of system limitations</li>
                        <li>Reduced blind acceptance of system recommendations</li>
                        <li>Early evidence of trust calibration as users questioned recommendations more thoughtfully</li>
                        <li>Identification of behavioral clusters that now inform adaptive UI/UX flows</li>
                    </ul>
                </section>

                <section class="project-section">
                    <h2>Scholarly Contribution</h2>
                    <p>This project refined my research agenda on trust drift over repeated human–AI interactions, explanation strategies tailored to user cognition, longitudinal measurement of autonomy and self-efficacy, and sociotechnical design for opportunity access.</p>
                </section>

                <p class="back-to-projects">← <a href="projects.html">Back to Projects &amp; Case Studies</a></p>
            </div>
        </div>
    </main>

    <div data-include="footer.html"></div>
    <script src="scripts.js"></script>
</body>
</html>
