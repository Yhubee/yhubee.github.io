<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Projects &amp; Case Studies | Ubong Eno Nkereuwem</title>
  <meta name="description" content="Projects and case studies spanning AI-guided advising, accessibility, sociotechnical systems, and ethical data work." />
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <header>
    <div class="navbar">
      <div class="brand">Ubong Eno Nkereuwem</div>
      <nav aria-label="Primary">
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="about.html">About</a></li>
          <li><a class="active" href="projects.html">Projects</a></li>
          <li><a href="research.html">Research</a></li>
          <li><a href="publications.html">Publications</a></li>
          <li><a href="teaching.html">Teaching</a></li>
          <li><a href="why.html">Why I Do This Work</a></li>
          <li><a href="bio.html">Bio</a></li>
        </ul>
      </nav>
    </div>
  </header>
  <main class="container">
    <h1>Projects &amp; Case Studies</h1>
    <p>My project work reflects a consistent thread: I use mixed-methods inquiry, sociotechnical reasoning, and ethical design practices to understand how intelligent systems influence, support, and sometimes interfere with human autonomy and decision-making. These case studies represent a blend of research, design, and computational awareness across EdTech, accessibility, and digital services.</p>

    <div class="section-divider"></div>

    <section class="card">
      <h3>1. Human–AI Learning Companion for International Students</h3>
      <p class="meta">Role: Technical AI Product Manager &amp; Human–Centered AI Researcher · Domains: Decision Support, Explanation Design, Trust Calibration, Digital Learning Ecosystems · Methods: Semi-structured interviews, cognitive mapping, interaction prototyping, behavioral analysis, explainability modeling</p>
      <h4>Overview</h4>
      <p>International students face complex educational decisions—program selection, visa processes, academic expectations, and career pathways—all while navigating unfamiliar cultural and institutional systems. These decisions involve uncertainty, high stakes, and emotional burden. My work on the AI Learning Companion examined how intelligent guidance systems can support clarity and confidence without inducing over-reliance.</p>
      <h4>Problem Framing</h4>
      <p>Most digital advising systems present recommendations without transparency. Students experiencing stress or limited familiarity with local academic structures often accept advice unquestioningly. The risk is twofold: over-reliance due to cognitive load, perceived authority, or system confidence; and erosion of agency when recommendations subtly nudge students toward predefined pathways. My goal was to design an AI system that supports, not supplants, human judgment.</p>
      <h4>What I Did</h4>
      <p><strong>Researching Real Help-Seeking Behaviors.</strong> I conducted interviews with international students to map uncertainty points, mental models of institutional processes, expectations of AI guidance, and perceived emotional or life stakes of decisions. Students frequently interpreted AI confidence as correctness, especially when overwhelmed—a pattern mirroring experiences like Maria’s and highlighting confidence bias in AI-mediated decision making.</p>
      <p><strong>Designing Adaptive Explanation Patterns.</strong> I co-developed an explanation schema that adjusted based on question type, risk level, user uncertainty signals, the amount of context the user provided, and whether the user asked for justification or alternatives. Examples included reasoned recommendations, confidence-qualified suggestions, multiple-pathway explanations, and limited knowledge disclosures to support healthy skepticism and informed choice.</p>
      <p><strong>Introducing Autonomy-Preserving Interaction Boundaries.</strong> I implemented user-managed control settings, value-sensitive defaults, rationale prompts, calls to double-check institutional requirements, and reflections like “Does this align with what you want?” grounded in FATE, sensemaking, and human–AI complementarity.</p>
      <h4>Outcomes</h4>
      <ul class="list">
        <li>Increased student engagement with learning and advising content</li>
        <li>Greater user awareness of system limitations</li>
        <li>Reduced blind acceptance of system recommendations</li>
        <li>Early evidence of trust calibration and identification of behavioral clusters that now inform adaptive UI/UX flows</li>
      </ul>
      <h4>Scholarly Contribution</h4>
      <ul class="list">
        <li>Trust drift over repeated human–AI interactions</li>
        <li>Explanation strategies tailored to user cognition</li>
        <li>Longitudinal measurement of autonomy and self-efficacy</li>
        <li>Sociotechnical design for opportunity access</li>
      </ul>
    </section>

    <section class="card">
      <h3>2. Fast Track Food Pickup Digital Service (MSc Dissertation)</h3>
      <p class="meta">Context: University of Roehampton, Distinction · Domains: Digital Queuing, Perceived Fairness, Service Transparency, Sociotechnical Systems · Methods: Mixed methods, interviews, surveys, thematic analysis, service blueprinting, UX evaluation</p>
      <h4>Overview</h4>
      <p>This research examined how digital interfaces influence perceptions of fairness, time, and service credibility. The project involved designing a digital Fast-Track pickup experience for a food service environment where bottlenecks and unclear feedback loops lowered overall user satisfaction.</p>
      <h4>Research Problem</h4>
      <p>Across many digital ordering systems, the biggest complaints relate not to actual time but perceived wait time shaped by transparency, environmental cues, interface feedback, and trust in digital-to-physical handoff. My research explored how perceived time, fairness, and trust are sociotechnically constructed.</p>
      <h4>What I Did</h4>
      <p><strong>Collected Mixed-Method Data.</strong> Surveys captured quantitative satisfaction indicators, interviews revealed qualitative expectations and psychological factors, and observational workflow mapping contextualized digital behaviors within physical service realities.</p>
      <p><strong>Evaluated Interface Trust Cues.</strong> Findings showed that time visibility improved perceived fairness; users interpreted ambiguous cues as incompetence; clear sequencing improved confidence; and a lack of queue transparency led to distrust even when service was fast.</p>
      <p><strong>Designed a Redesigned Service Blueprint.</strong> The new blueprint provided prediction-based wait estimates, transparent service states, consistent feedback signage, and UX patterns inspired by reliability and clarity principles, projecting ~35 percent improvement in perceived wait time.</p>
      <h4>Scholarly Contribution</h4>
      <ul class="list">
        <li>Sociotechnical fairness</li>
        <li>Digital representation of physical systems</li>
        <li>Psychological construction of time and waiting</li>
        <li>Trust as a hybrid of interface cues and environmental signals</li>
      </ul>
    </section>

    <section class="card">
      <h3>3. AI Accessibility Feature for Sky (Professional Certificate Capstone)</h3>
      <p class="meta">Context: King’s College London · Domains: Accessibility, AI Personalization, Inclusive Media Design, Regulation (WCAG, Ofcom) · Methods: Accessibility heuristics, user story mapping, collaborative prototyping, regulatory analysis</p>
      <h4>Overview</h4>
      <p>In collaboration with Sky, I worked on designing an AI-powered media accessibility feature that adapted content presentation based on user needs while respecting regulatory frameworks.</p>
      <h4>What I Did</h4>
      <p><strong>Mapped Accessibility Needs.</strong> Analyzed needs of users with sensory sensitivities, cognitive differences, and reading or comprehension challenges.</p>
      <p><strong>Developed Inclusive User Stories.</strong> Stories emphasized control, comprehension, comfort, transparency and consent, and the ability to override automated adjustments.</p>
      <p><strong>Balanced Personalization With User Autonomy.</strong> I foregrounded user-set preferences, limited unsolicited adjustments, ensured explanations of automated behaviors, and enabled predictable interaction patterns to answer whether AI can personalize without becoming intrusive or paternalistic.</p>
      <h4>Impact + Research Value</h4>
      <p>This project revealed how accessibility, personalization, and value-sensitive design intersect. It strengthened my interest in cross-cultural accessibility, adaptive AI for diverse cognitive styles, and ethical personalization frameworks.</p>
    </section>

    <section class="card">
      <h3>4. CSR Data Annotation &amp; Ethical Dataset Construction (Isahit)</h3>
      <p class="meta">Domains: Dataset documentation, ambiguity resolution, bias detection, CSR thematic analysis · Methods: Annotation studies, content categorization, reliability checks</p>
      <p>This work gave me hands-on experience with dataset ambiguity, annotation inconsistency, and the human labor behind AI data pipelines. I contributed to categorizing CSR-related content for training AI models and saw how labeling decisions propagate into model behavior, reinforcing my commitment to interpretability and ethical responsibility in data preprocessing.</p>
    </section>

    <section class="card">
      <h3>5. Digital Equity &amp; Development Research (Dataville)</h3>
      <p class="meta">Domains: ICT4D, AI for Social Good, Development Studies · Methods: Mixed methods, policy review, contextual analysis</p>
      <p>Explored how technology, policy, and sociotechnical context shape global inequities. This broadened my understanding of the systemic forces that influence AI access and fairness.</p>
    </section>

    <section class="card">
      <h3>6. Data, Insight &amp; Behavioral Analysis Work</h3>
      <p>Across multiple roles, I conducted SQL/Python behavioral analysis, interaction pattern mining, service usage segmentation, simple predictive modeling to understand risk or uncertainty clusters, and dashboard design for non-technical stakeholders. These experiences strengthened my analytical lens and helped me frame AI design questions in empirical terms.</p>
    </section>
  </main>
  <footer class="footer">
    <div class="container">
      <div class="small">© Ubong Eno Nkereuwem</div>
      <div class="small">Email: <a href="mailto:ubongenonkereuwem@gmail.com">ubongenonkereuwem@gmail.com</a> · <a href="https://www.linkedin.com/in/ubongeno" target="_blank" rel="noreferrer">LinkedIn</a></div>
    </div>
  </footer>
</body>
</html>
