<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="About Ubong Eno Nkereuwem - Human-Centered AI Researcher exploring trust, autonomy, and sociotechnical systems.">
    <meta name="author" content="Ubong Eno Nkereuwem">
    <title>About Me | Ubong Eno Nkereuwem</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <a href="index.html" class="logo">Ubong Eno Nkereuwem</a>
            <nav>
                <a href="index.html">Home</a>
                <a href="about.html" class="active">About</a>
                <a href="projects.html">Projects</a>
                <a href="research.html">Research</a>
                <a href="teaching.html">Teaching</a>
                <a href="why.html">Why This Work</a>
            </nav>
        </div>
    </header>

    <div class="page-header" data-animate>
        <div class="container" data-animate>
            <h1>About Me</h1>
            <p class="lead">My journey into Human-Centered AI research</p>
        </div>
    </div>

    <main>
        <div class="container">
            <div class="content-wrapper">
                <section class="about-intro" data-animate>
                    <div>
                        <img src="content/assets/ubong-eno.jpeg" alt="Ubong Eno Nkereuwem" class="about-image">
                    </div>
                    <div>
                        <p>My work in Human–Centered AI is anchored in a simple but profound observation: <strong>intelligent systems shape human behavior in ways that often go unnoticed</strong>, especially for users navigating uncertainty, cognitive load, or systemic constraints.</p>
                        
                        <p>This insight emerged during a usability testing session I conducted with a neurodivergent user, whom I call Maria. She was autistic and relied on consistency and predictability to feel anchored when interacting with digital systems. During the session, she smiled each time the AI assistant behaved as expected. But when the system generated an unexpected suggestion, she hesitated. Over multiple interactions, something subtle yet unsettling happened: she began to defer to the system's confidence even when its recommendations clearly contradicted her personal preferences.</p>
                    </div>
                </section>

                <p>That moment shifted my trajectory. It revealed how an intelligent system's aura of certainty can quietly displace human judgment, eroding autonomy not through force but through design. I realized that if AI is going to participate in everyday decision-making, it must be intentionally designed to reinforce—not override—human agency.</p>

                <hr>

                <h2 data-animate>Applied Work</h2>
                <p>This realization guided my work as a Technical AI Product Manager. At Jourhney, I help build AI-driven learning and decision-support tools for global learners navigating complex academic, immigration, and career choices. These are not trivial contexts; they involve high emotional stakes, fragmented information, and substantial cognitive burden. In these systems, algorithmic misalignment can shape life trajectories.</p>

                <p>To address this, I focus on designing interactions that foreground:</p>
                <ul>
                    <li>transparent rationale</li>
                    <li>value-sensitive defaults</li>
                    <li>user-managed control boundaries</li>
                    <li>adaptive explanation cues</li>
                    <li>mechanisms that make system limitations visible</li>
                    <li>interactions that center user agency</li>
                </ul>

                <p>Through this work, I learned that ethical UX interventions—such as offering explanations or surfacing uncertainty—are necessary but insufficient. Industry often implements isolated ethical features without frameworks to measure their long-term impact on autonomy, trust calibration, mental model formation, and decision trajectories. This recognition became the intellectual foundation for my research aspirations: <strong>to develop empirical, longitudinal methods for evaluating how AI design influences human autonomy over time.</strong></p>

                <hr>

                <h2 data-animate>Academic Foundation</h2>
                <p>Alongside applied work, I strengthened my research foundation academically.</p>

                <div class="card" data-animate>
                    <h4>MSc, University of Roehampton (Distinction)</h4>
                    <p>During my Master's degree at the University of Roehampton, I studied how organizations navigate ethical responsibility within digital innovation. My dissertation, which redesigned a digital food pickup system, explored how perceived time, interface transparency, and digital-predicted behavior shape trust and fairness evaluations. This experience revealed the complexity of sociotechnical trust: what users perceive as "fair" is often constructed through digital cues rather than objective metrics.</p>
                </div>

                <div class="card" data-animate>
                    <h4>Product Management Professional Certificate, King's College London</h4>
                    <p>At King's College London, my Product Management program deepened my technical fluency in Python, SQL, and data modeling, and provided the methodological grounding to participate in AI design with both computational awareness and human-centered sensitivity. My capstone project with Sky—a WCAG/Ofcom-aligned AI accessibility feature—made clear how inclusion, autonomy, and algorithmic personalization must be approached as intertwined design considerations rather than separate layers.</p>
                </div>

                <hr>

                <h2 data-animate>Community Impact</h2>
                <p>Beyond formal education, I remain committed to knowledge-sharing as a form of community impact. I have mentored learners, led youth programs, and taught literacy and life-skills courses to students in Nigeria. These experiences continuously remind me that users of AI systems bring diverse cultural, emotional, and cognitive realities into their interactions. <strong>Designing AI for "average users" is not enough. Designing AI for real people, in real contexts, is the only ethical path.</strong></p>

                <p>I also use writing—including a public-facing newsletter that explains AI ethics, HCI principles, and trust-aware design—to make these concepts accessible to early-career technologists and builders worldwide.</p>

                <hr>

                <h2 data-animate>Current Directions</h2>
                <p>Today, my work spans three interconnected directions:</p>
                <ol>
                    <li><strong>Understanding how humans form and update mental models of AI systems</strong>, especially under uncertainty.</li>
                    <li><strong>Designing adaptive explanations and trust-calibrating interfaces</strong> that respond to user context, literacy, and goals.</li>
                    <li><strong>Investigating how autonomy, confidence, and value alignment evolve longitudinally</strong> during repeated human–AI interactions.</li>
                </ol>

                <p>My long-term goal is to help build a future where intelligent systems enhance human capability, preserve dignity, and expand access to opportunity across diverse and global contexts. I want to contribute to the theory, methods, and frameworks that ensure AI strengthens—not replaces—human judgment.</p>

                <hr>

                <h2 data-animate>Academic Bios</h2>

                <div class="bio-card" data-animate>
                    <h4>Micro Bio</h4>
                    <p class="word-count">(25 words)</p>
                    <p>Ubong Eno Nkereuwem researches Human–Centered AI, trust calibration, and sociotechnical systems. She studies how intelligent systems influence autonomy, decision-making, and opportunity access across educational and accessibility contexts.</p>
                </div>

                <div class="bio-card" data-animate>
                    <h4>Short Academic Bio</h4>
                    <p class="word-count">(100 words)</p>
                    <p>Ubong Eno Nkereuwem is a Human–Centered AI researcher and Technical AI Product Manager exploring how people build trust, form mental models, and maintain autonomy when interacting with intelligent systems. Her work examines AI-guided decision-making, transparency mechanisms, and adaptive explanations in complex sociotechnical environments, particularly in education and accessibility. She brings experience designing ethical AI features, value-sensitive defaults, and explainability tools at Jourhney and Sky, supported by research in CSR dataset annotation and digital services. Using mixed methods and sociotechnical analysis, she studies how AI systems can reinforce human judgment rather than displace it.</p>
                </div>

                <div class="bio-card" data-animate>
                    <h4>Full Academic Bio</h4>
                    <p class="word-count">(200 words)</p>
                    <p>Ubong Eno Nkereuwem is a researcher in Human–Centered AI whose work focuses on trust calibration, human autonomy, and sociotechnical systems. She studies how people interpret, rely on, and sometimes over-depend on intelligent systems, particularly in educational and accessibility contexts. Her interest in this problem emerged during applied research sessions where she observed vulnerable users gradually deferring to AI recommendations simply because the system presented itself as confident. This experience shaped her commitment to designing intelligent systems that reinforce user autonomy and support responsible decision-making.</p>
                    <p>Her professional work as a Technical AI Product Manager spans AI-supported learning tools, adaptive guidance systems, transparency mechanisms, and ethical UX interventions. She brings technical fluency in Python, SQL, and data modeling, alongside experience in UX research and sociotechnical evaluation. Her academic foundation includes a Distinction MSc from the University of Roehampton and a Product Management Professional Certificate from King's College London.</p>
                    <p>Her research investigates adaptive explanations, mental model formation, and longitudinal autonomy in human–AI interaction. Ultimately, she aims to develop frameworks that ensure AI systems strengthen human reasoning, preserve dignity, and remain aligned with evolving human values across diverse contexts.</p>
                </div>

                <div class="contact-info" data-animate>
                    <h3>Get in Touch</h3>
                    <p>
                        <a href="mailto:ubongenonkereuwem@gmail.com">ubongenonkereuwem@gmail.com</a>
                        <br>
                        <a href="https://www.linkedin.com/in/ubongeno" target="_blank" rel="noopener noreferrer">LinkedIn: linkedin.com/in/ubongeno</a>
                    </p>
                </div>
            </div>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2024 Ubong Eno Nkereuwem. All rights reserved.</p>
            <p>
                <a href="mailto:ubongenonkereuwem@gmail.com">Email</a> ·
                <a href="https://www.linkedin.com/in/ubongeno" target="_blank" rel="noopener noreferrer">LinkedIn</a>
            </p>
        </div>
    </footer>
    <script src="animations.js" defer></script>
</body>
</html>
