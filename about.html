<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>About | Ubong Eno Nkereuwem</title>
  <meta name="description" content="About Ubong Eno Nkereuwem – Human-Centered AI researcher focused on autonomy, trust, and sociotechnical systems." />
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <header>
    <div class="navbar">
      <div class="brand">Ubong Eno Nkereuwem</div>
      <nav aria-label="Primary">
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a class="active" href="about.html">About</a></li>
          <li><a href="projects.html">Projects</a></li>
          <li><a href="research.html">Research</a></li>
          <li><a href="publications.html">Publications</a></li>
          <li><a href="teaching.html">Teaching</a></li>
          <li><a href="why.html">Why I Do This Work</a></li>
          <li><a href="bio.html">Bio</a></li>
        </ul>
      </nav>
    </div>
  </header>
  <main class="container">
    <h1>About Me</h1>
    <div class="hero">
      <div>
        <p>My work in Human–Centered AI is anchored in a simple but profound observation: intelligent systems shape human behavior in ways that often go unnoticed, especially for users navigating uncertainty, cognitive load, or systemic constraints.</p>
        <p>This insight emerged during a usability testing session I conducted with a neurodivergent user, whom I call Maria. She was autistic and relied on consistency and predictability to feel anchored when interacting with digital systems. During the session, she smiled each time the AI assistant behaved as expected. But when the system generated an unexpected suggestion, she hesitated. Over multiple interactions, she began to defer to the system’s confidence even when its recommendations clearly contradicted her personal preferences.</p>
        <p>That moment shifted my trajectory. It revealed how an intelligent system’s aura of certainty can quietly displace human judgment, eroding autonomy not through force but through design. I realized that if AI is going to participate in everyday decision-making, it must be intentionally designed to reinforce—not override—human agency.</p>
      </div>
      <div>
        <img src="content/assets/ubong-eno.jpeg" alt="Ubong Eno Nkereuwem speaking" />
      </div>
    </div>

    <div class="section-divider"></div>

    <section>
      <h2>Professional Practice</h2>
      <p>This realization guided my work as a Technical AI Product Manager. At Jourhney, I help build AI-driven learning and decision-support tools for global learners navigating complex academic, immigration, and career choices—contexts involving high emotional stakes, fragmented information, and substantial cognitive burden. Algorithmic misalignment can shape life trajectories.</p>
      <p>To address this, I focus on designing interactions that foreground transparent rationale, value-sensitive defaults, user-managed control boundaries, adaptive explanation cues, mechanisms that make system limitations visible, and interactions that center user agency.</p>
      <p>Through this work, I learned that ethical UX interventions—such as offering explanations or surfacing uncertainty—are necessary but insufficient. Industry often implements isolated ethical features without frameworks to measure their long-term impact on autonomy, trust calibration, mental model formation, and decision trajectories. This recognition became the intellectual foundation for my research aspirations.</p>
    </section>

    <section>
      <h2>Academic Formation</h2>
      <p>During my Master’s degree at the University of Roehampton, I studied how organizations navigate ethical responsibility within digital innovation. My dissertation, which redesigned a digital food pickup system, explored how perceived time, interface transparency, and digital-predicted behavior shape trust and fairness evaluations. This experience revealed the complexity of sociotechnical trust: what users perceive as “fair” is often constructed through digital cues rather than objective metrics.</p>
      <p>At King’s College London, my Product Management program deepened my technical fluency in Python, SQL, and data modeling, and provided the methodological grounding to participate in AI design with both computational awareness and human-centered sensitivity. My capstone project with Sky—a WCAG/Ofcom-aligned AI accessibility feature—made clear how inclusion, autonomy, and algorithmic personalization must be approached as intertwined design considerations rather than separate layers.</p>
    </section>

    <section>
      <h2>Community &amp; Knowledge Sharing</h2>
      <p>Beyond formal education, I remain committed to knowledge-sharing as a form of community impact. I have mentored learners, led youth programs, and taught literacy and life-skills courses to students in Nigeria. These experiences continuously remind me that users of AI systems bring diverse cultural, emotional, and cognitive realities into their interactions. Designing AI for “average users” is not enough. Designing AI for real people, in real contexts, is the only ethical path.</p>
      <p>I also use writing—including a public-facing newsletter that explains AI ethics, HCI principles, and trust-aware design—to make these concepts accessible to early-career technologists and builders worldwide.</p>
    </section>

    <section>
      <h2>Current Focus</h2>
      <p>Today, my work spans three interconnected directions:</p>
      <ul class="list">
        <li>Understanding how humans form and update mental models of AI systems, especially under uncertainty.</li>
        <li>Designing adaptive explanations and trust-calibrating interfaces that respond to user context, literacy, and goals.</li>
        <li>Investigating how autonomy, confidence, and value alignment evolve longitudinally during repeated human–AI interactions.</li>
      </ul>
      <p>My long-term goal is to help build a future where intelligent systems enhance human capability, preserve dignity, and expand access to opportunity across diverse and global contexts. I want to contribute to the theory, methods, and frameworks that ensure AI strengthens—not replaces—human judgment.</p>
    </section>
  </main>
  <footer class="footer">
    <div class="container">
      <div class="small">© Ubong Eno Nkereuwem</div>
      <div class="small">Email: <a href="mailto:ubongenonkereuwem@gmail.com">ubongenonkereuwem@gmail.com</a> · <a href="https://www.linkedin.com/in/ubongeno" target="_blank" rel="noreferrer">LinkedIn</a></div>
    </div>
  </footer>
</body>
</html>
